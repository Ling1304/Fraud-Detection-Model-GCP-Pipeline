{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44dbdd13",
   "metadata": {},
   "source": [
    "# 1.0 Install and Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91315d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install kfp\n",
    "# !pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7869ee7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp version: 2.9.0\n",
      "google.cloud.aiplatform version: 1.71.1\n"
     ]
    }
   ],
   "source": [
    "# IMPORT REQUIRED LIBRARIES\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        Markdown,\n",
    "                        HTML,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "from kfp import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "import kfp\n",
    "import google.cloud.aiplatform \n",
    "print(f\"kfp version: {kfp.__version__}\")\n",
    "print(f\"google.cloud.aiplatform version: {google.cloud.aiplatform.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf6b87",
   "metadata": {},
   "source": [
    "# 2.0 Define the Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d0863",
   "metadata": {},
   "source": [
    "## 2.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733dfda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hezron Ling\\AppData\\Local\\Temp\\ipykernel_6096\\1102930185.py:3: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n",
      "C:\\Users\\Hezron Ling\\AppData\\Local\\Temp\\ipykernel_6096\\1102930185.py:7: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  def data_cleaning(\n"
     ]
    }
   ],
   "source": [
    "# Set the docker image \n",
    "BASE_IMAGE = \"eu.gcr.io/norse-voice-440615-v3/practice_gcp@sha256:506a11f1f5ded3bc2c4fb97f560e7aedf91a89dcc53f4b730c9ca4b8de6a777d\"\n",
    "\n",
    "@component(\n",
    "    base_image=BASE_IMAGE, # Load the docker image\n",
    "    output_component_file=\"data_cleaning.yaml\" # Output filename\n",
    ")\n",
    "def data_cleaning(\n",
    "    output_dir: Output[Dataset], # Output path\n",
    "):\n",
    "\n",
    "    from src.preprocess import data_cleaning\n",
    "    \n",
    "    # Clean the data\n",
    "    df = data_cleaning()\n",
    "    print(\"Data is cleaned\")\n",
    "    \n",
    "    # Save dataframe as CSV\n",
    "    df.to_csv(output_dir.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7e3f4",
   "metadata": {},
   "source": [
    "## 2.2 Data Processing\n",
    "- Feature Engineering \n",
    "- Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf0f585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hezron Ling\\AppData\\Local\\Temp\\ipykernel_6096\\317081645.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n",
      "C:\\Users\\Hezron Ling\\AppData\\Local\\Temp\\ipykernel_6096\\317081645.py:5: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  def data_processing(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE, # Load the docker image\n",
    "    output_component_file=\"data_processing.yaml\", # Output filename\n",
    ")\n",
    "def data_processing(\n",
    "    dataset_full: Input[Dataset], # Input the cleaned dataset\n",
    "    dataset_processed: Output[Dataset]): # Output processed dataset\n",
    "\n",
    "    import pandas as pd\n",
    "    from src.preprocess import data_preprocessing\n",
    "\n",
    "    df = pd.read_csv(dataset_full.path)\n",
    "    \n",
    "    # Process the dataframe\n",
    "    df_processed = data_preprocessing(df)\n",
    "    print(\"Performed Feature Engineering and Label Encoding\")\n",
    "    \n",
    "    # Save processed dataframe as CSV\n",
    "    df_processed.to_csv(dataset_processed.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b43a7",
   "metadata": {},
   "source": [
    "## 2.3 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd8be1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hezron Ling\\AppData\\Local\\Temp\\ipykernel_6096\\1835865731.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n",
      "C:\\Users\\Hezron Ling\\AppData\\Local\\Temp\\ipykernel_6096\\1835865731.py:5: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  def train_model(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE, # Load the docker image\n",
    "    output_component_file=\"train_model.yaml\", # Output filename\n",
    ")\n",
    "def train_model(\n",
    "    dataset_processed: Input[Dataset], # Input processed dataset\n",
    "    model: Output[Model] # Output model scores\n",
    "):\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from src.train import train_and_evaluate\n",
    "    \n",
    "    # Load the processed dataset\n",
    "    df = pd.read_csv(dataset_processed.path)\n",
    "    \n",
    "    # Pass `model.path` to save the trained model to that path\n",
    "    outputs = train_and_evaluate(df, model.path)\n",
    "    scores = outputs['scores']\n",
    "    \n",
    "    # Save the evaluation scores in json file\n",
    "    with open(model.path + \"_scores.json\", 'w') as f:\n",
    "        json.dump(scores, f)\n",
    "\n",
    "    # Print the path to the model and the model scores\n",
    "    print(f\"Model artifact saved at: {model.path}\")\n",
    "    print(f\"Scores saved at: {model.path}_scores.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34012108",
   "metadata": {},
   "source": [
    "## 2.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bbdc347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hezron Ling\\AppData\\Local\\Temp\\ipykernel_6096\\1926838823.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n",
      "C:\\Users\\Hezron Ling\\AppData\\Local\\Temp\\ipykernel_6096\\1926838823.py:5: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  def compute_metrics(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE, # Load the docker image\n",
    "    output_component_file=\"compute_metrics.yaml\", # Output filename\n",
    ")\n",
    "def compute_metrics(\n",
    "    model: Input[Model], # Input the trained model\n",
    "    train_metric: Output[Metrics], # Output training score\n",
    "    test_metric: Output[Metrics], # Output testing score\n",
    "    cv_metric: Output[Metrics] # Output cross validation score (5 folds)\n",
    "):\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    # Define the path to the scores JSON file\n",
    "    scores_file_name = model.path + \"_scores.json\"\n",
    "    \n",
    "    # Open and load the scores from the JSON file\n",
    "    with open(scores_file_name, 'r') as file:  \n",
    "        model_metrics = json.load(file)\n",
    "        \n",
    "    # Log the metrics for train, test, and cross-validation AUC\n",
    "    train_metric.log_metric('train_auc', model_metrics['train'])\n",
    "    test_metric.log_metric('test_auc', model_metrics['test'])\n",
    "    cv_metric.log_metric('cv_auc', model_metrics['cross_validation (5 folds)'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637491c9",
   "metadata": {},
   "source": [
    "# 3.0 Define and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b165614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Google Application Credential (JSON Key)\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/Hezron Ling/Desktop/Practice_GCP/norse-voice-440615-v3-e1e314d6a20e.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "257209e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/asia-southeast2/pipelines/runs/pipeline-practicegcp-20241210174215?project=120166958921\n",
      "PipelineJob projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/120166958921/locations/asia-southeast2/pipelineJobs/pipeline-practicegcp-20241210174215\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Use timestamp to define the name of each pipeline\n",
    "TIMESTAMP = dt.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'pipeline-practice_gcp-{}'.format(TIMESTAMP)\n",
    "BUCKET_NAME = 'norse-voice-440615-v3_cloudbuild'\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/\"\n",
    "REGION = \"asia-southeast2\"\n",
    "PROJECT_ID = \"norse-voice-440615-v3\"\n",
    "\n",
    "# Define the pipeline, each component reuses outputs from previous component\n",
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline\n",
    "    name=\"pipeline-practiceGCP\"   \n",
    ")\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID, \n",
    "    region: str = REGION, \n",
    "    display_name: str = DISPLAY_NAME\n",
    "):\n",
    "    \n",
    "    # Data cleaning\n",
    "    data_cleaning_op = data_cleaning()\n",
    "    \n",
    "    # Data processing\n",
    "    data_processing_op = data_processing(\n",
    "        dataset_full=data_cleaning_op.outputs[\"output_dir\"]\n",
    "    )\n",
    "    \n",
    "    # Model training\n",
    "    train_model_op = train_model(\n",
    "        dataset_processed=data_processing_op.outputs[\"dataset_processed\"]\n",
    "    ).set_cpu_limit('8').set_memory_limit('32G') # Increase CPU and Memory \n",
    "    \n",
    "    # Model evaluation\n",
    "    model_evaluation_op = compute_metrics(\n",
    "        model=train_model_op.outputs[\"model\"]\n",
    "    )\n",
    "\n",
    "# Compile the pipeline as JSON file\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='practiceGCP_pipeline.json'\n",
    ")\n",
    "\n",
    "# Start running the pipeline\n",
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"practiceGCP-pipeline\",\n",
    "    template_path=\"practiceGCP_pipeline.json\",\n",
    "    enable_caching=False,\n",
    "    location=REGION,\n",
    "    project=PROJECT_ID\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "start_pipeline.run(service_account=\"120166958921-compute@developer.gserviceaccount.com\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
